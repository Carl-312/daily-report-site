"""
Daily Report Site - Main Entry Point
Unified CLI for fetching news, summarizing, and building static site
"""
from __future__ import annotations
import argparse
import sys

from config import get_config
from sources import fetch_all, Article
from utils import dedupe, today_ymd, today_cn, save_json, save_markdown, load_json
from summarizer import summarize, offline_summary, test_connection


def cmd_run(args):
    """Full pipeline: fetch â†’ summarize â†’ build"""
    cfg = get_config()
    date_str = today_ymd()
    
    print(f"ğŸš€ Daily Report - {date_str}")
    print("=" * 50)
    
    # 1. Fetch
    print("\nğŸ“¡ Fetching news...")
    articles = fetch_all(
        enabled_sources=cfg.sources,
        max_articles=cfg.max_articles,
        syft_url=cfg.syft_web_app_url,
        syft_key=cfg.syft_secret_key,
    )
    
    # 2. Dedupe
    print(f"\nğŸ”„ Deduplicating {len(articles)} articles...")
    articles = dedupe(articles)
    print(f"   Remaining: {len(articles)} unique articles")
    
    # 3. Save JSON
    articles_dict = [a.to_dict() if isinstance(a, Article) else a for a in articles]
    json_path = save_json(cfg.data_dir, date_str, {
        "date": date_str,
        "articles": articles_dict,
    })
    print(f"\nğŸ’¾ Saved JSON: {json_path}")
    
    # 4. Summarize
    print("\nğŸ¤– Generating summary...")
    if args.offline or not cfg.api_key:
        if not cfg.api_key:
            print("   âš ï¸  No API key, using offline mode")
        content = offline_summary(articles_dict)
    else:
        content = summarize(articles_dict, stream=True)
    
    # 5. Build Markdown title
    title = f"ğŸ”¥ï¼ˆ{today_cn()}ï¼‰æ¯æ—¥AIèµ„è®¯ä¸€è§ˆâœ¨"
    full_content = f"{title}\n\n{content}"
    
    # 6. Save Markdown
    md_path = save_markdown(cfg.content_dir, date_str, full_content)
    print(f"\nğŸ“ Saved Markdown: {md_path}")
    
    # 7. Build HTML
    print("\nğŸ—ï¸  Building HTML site...")
    from build import build_site
    build_site()
    
    print("\n" + "=" * 50)
    print("âœ… Done!")
    print(f"   Articles: {len(articles)}")
    print(f"   JSON: {json_path}")
    print(f"   Markdown: {md_path}")
    print(f"   HTML: {cfg.docs_dir}/")


def cmd_fetch(args):
    """Fetch only - save to JSON"""
    cfg = get_config()
    date_str = today_ymd()
    
    print(f"ğŸ“¡ Fetching news for {date_str}...")
    articles = fetch_all(
        enabled_sources=cfg.sources,
        max_articles=cfg.max_articles,
        syft_url=cfg.syft_web_app_url,
        syft_key=cfg.syft_secret_key,
    )
    
    articles = dedupe(articles)
    articles_dict = [a.to_dict() if isinstance(a, Article) else a for a in articles]
    
    json_path = save_json(cfg.data_dir, date_str, {
        "date": date_str,
        "articles": articles_dict,
    })
    
    print(f"âœ… Saved {len(articles)} articles to {json_path}")


def cmd_summarize(args):
    """Summarize only - from existing JSON"""
    cfg = get_config()
    date_str = today_ymd()
    
    data = load_json(cfg.data_dir, date_str)
    if not data:
        print(f"âŒ No data found for {date_str}")
        return
    
    articles = data.get("articles", [])
    print(f"ğŸ¤– Summarizing {len(articles)} articles...")
    
    if args.offline or not cfg.api_key:
        content = offline_summary(articles)
    else:
        content = summarize(articles, stream=True)
    
    title = f"ğŸ”¥ï¼ˆ{today_cn()}ï¼‰æ¯æ—¥AIèµ„è®¯ä¸€è§ˆâœ¨"
    full_content = f"{title}\n\n{content}"
    
    md_path = save_markdown(cfg.content_dir, date_str, full_content)
    print(f"âœ… Saved to {md_path}")


def cmd_build(args):
    """Build HTML site only"""
    print("ğŸ—ï¸  Building HTML site...")
    from build import build_site
    build_site()


def cmd_test(args):
    """Test API connection"""
    test_connection()


def main():
    parser = argparse.ArgumentParser(
        description="Daily Report Site - AI News Aggregator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Commands")
    
    # run command
    p_run = subparsers.add_parser("run", help="Full pipeline: fetch â†’ summarize â†’ build")
    p_run.add_argument("--offline", action="store_true", help="Use offline summarization")
    p_run.set_defaults(func=cmd_run)
    
    # fetch command
    p_fetch = subparsers.add_parser("fetch", help="Fetch news only")
    p_fetch.set_defaults(func=cmd_fetch)
    
    # summarize command
    p_sum = subparsers.add_parser("summarize", help="Summarize from existing JSON")
    p_sum.add_argument("--offline", action="store_true", help="Use offline summarization")
    p_sum.set_defaults(func=cmd_summarize)
    
    # build command
    p_build = subparsers.add_parser("build", help="Build HTML site only")
    p_build.set_defaults(func=cmd_build)
    
    # test command
    p_test = subparsers.add_parser("test", help="Test API connection")
    p_test.set_defaults(func=cmd_test)
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        sys.exit(1)
    
    args.func(args)


if __name__ == "__main__":
    main()

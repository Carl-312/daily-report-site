# æ‰©å±•æ–°é—»æºæ•™ç¨‹

å­¦ä¹ å¦‚ä½•ä¸º Daily Report Site æ·»åŠ è‡ªå®šä¹‰æ–°é—»æºã€‚

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿ:
- âœ… ç†è§£æ–°é—»æºæ¥å£è§„èŒƒ
- âœ… åˆ›å»ºè‡ªå®šä¹‰æ–°é—»æºæ¨¡å—
- âœ… æ³¨å†Œå¹¶å¯ç”¨æ–°é—»æº
- âœ… æµ‹è¯•å’Œè°ƒè¯•æ–°é—»æº

---

## ğŸ“‹ å‰ç½®çŸ¥è¯†

- Python åŸºç¡€è¯­æ³•
- HTTP è¯·æ±‚ (`requests` åº“)
- HTML è§£æ (`BeautifulSoup`)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹: æ·»åŠ  Hacker News

### ç¬¬ä¸€æ­¥: åˆ›å»ºæ¨¡å—æ–‡ä»¶

åœ¨ `sources/` ç›®å½•åˆ›å»º `hackernews.py`:

```python
"""
Hacker News Source Scraper
"""
import requests
from typing import List, Dict


def fetch() -> List[Dict[str, str]]:
    """
    ä» Hacker News è·å–å¤´æ¡æ–°é—»
    
    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    articles = []
    
    # 1. è·å–å¤´æ¡ ID åˆ—è¡¨
    response = requests.get(
        "https://hacker-news.firebaseio.com/v0/topstories.json",
        timeout=10
    )
    response.raise_for_status()
    story_ids = response.json()[:10]  # ä»…å–å‰ 10 æ¡
    
    # 2. è·å–æ¯æ¡æ–°é—»è¯¦æƒ…
    for story_id in story_ids:
        try:
            story_resp = requests.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                timeout=5
            )
            story_resp.raise_for_status()
            story = story_resp.json()
            
            # 3. æ ¼å¼åŒ–ä¸ºæ ‡å‡†æ ¼å¼
            articles.append({
                "title": story.get("title", "Untitled"),
                "link": story.get("url", f"https://news.ycombinator.com/item?id={story_id}"),
                "desc": story.get("text", "No description available")[:200],
            })
        except Exception as e:
            print(f"   âš ï¸  Failed to fetch story {story_id}: {e}")
            continue
    
    return articles
```

### ç¬¬äºŒæ­¥: æ³¨å†Œæ–°é—»æº

ç¼–è¾‘ `sources/__init__.py`ï¼Œæ·»åŠ æ³¨å†Œ:

```python
# sources/__init__.py
from .aibase import fetch as fetch_aibase
from .techcrunch import fetch as fetch_techcrunch
from .theverge import fetch as fetch_theverge
from .hackernews import fetch as fetch_hackernews  # æ–°å¢

SOURCE_REGISTRY = {
    "aibase": fetch_aibase,
    "techcrunch": fetch_techcrunch,
    "theverge": fetch_theverge,
    "hackernews": fetch_hackernews,  # æ–°å¢
}
```

### ç¬¬ä¸‰æ­¥: å¯ç”¨æ–°é—»æº

ç¼–è¾‘ `config.yaml`:

```yaml
sources:
  aibase: true
  techcrunch: true
  theverge: true
  hackernews: true  # å¯ç”¨
```

### ç¬¬å››æ­¥: æµ‹è¯•

```bash
# å®Œæ•´æµ‹è¯•
python main.py run --offline

# ä»…æµ‹è¯•æŠ“å–
python main.py fetch
```

**æŸ¥çœ‹è¾“å‡º**:
```
ğŸ“¡ Fetching news...
   AIBase: 8 articles
   TechCrunch: 5 articles
   The Verge: 6 articles
   HackerNews: 10 articles  # æ–°å¢
```

---

## ğŸ“š è¿›é˜¶ç¤ºä¾‹

### ç¤ºä¾‹ 1: RSS Feed æ–°é—»æº (Ars Technica)

```python
# sources/arstechnica.py
"""
Ars Technica RSS Feed Scraper
"""
import feedparser
from typing import List, Dict


def fetch() -> List[Dict[str, str]]:
    """ä» Ars Technica RSS è·å–æ–°é—»"""
    feed_url = "https://feeds.arstechnica.com/arstechnica/index"
    
    feed = feedparser.parse(feed_url)
    articles = []
    
    for entry in feed.entries[:15]:  # é™åˆ¶ 15 æ¡
        articles.append({
            "title": entry.title,
            "link": entry.link,
            "desc": entry.get("summary", "")[:300],
        })
    
    return articles
```

**ä¾èµ–**: éœ€è¦å®‰è£… `feedparser`

```bash
pip install feedparser
# æ›´æ–° requirements.txt
pip freeze | grep feedparser >> requirements.txt
```

### ç¤ºä¾‹ 2: éœ€è¦è®¤è¯çš„ API

```python
# sources/newsapi.py
"""
NewsAPI.org Integration
"""
import os
import requests
from typing import List, Dict


def fetch() -> List[Dict[str, str]]:
    """ä» NewsAPI è·å–ç§‘æŠ€æ–°é—»"""
    api_key = os.getenv("NEWSAPI_KEY")
    if not api_key:
        print("   âš ï¸  NEWSAPI_KEY not configured, skipping")
        return []
    
    response = requests.get(
        "https://newsapi.org/v2/top-headlines",
        params={
            "category": "technology",
            "language": "en",
            "apiKey": api_key,
            "pageSize": 20,
        },
        timeout=10
    )
    response.raise_for_status()
    data = response.json()
    
    articles = []
    for article in data.get("articles", []):
        articles.append({
            "title": article["title"],
            "link": article["url"],
            "desc": article.get("description", "")[:300],
        })
    
    return articles
```

**é…ç½®**:

```bash
# .env
NEWSAPI_KEY=your-newsapi-key
```

### ç¤ºä¾‹ 3: HTML è§£æ (ä½¿ç”¨ BeautifulSoup)

```python
# sources/producthunt.py
"""
Product Hunt Scraper
"""
import requests
from bs4 import BeautifulSoup
from typing import List, Dict


def fetch() -> List[Dict[str, str]]:
    """ä» Product Hunt è·å–çƒ­é—¨äº§å“"""
    url = "https://www.producthunt.com/"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.text, "html.parser")
    articles = []
    
    # æŸ¥æ‰¾äº§å“å¡ç‰‡ (æ ¹æ®å®é™… HTML ç»“æ„è°ƒæ•´)
    products = soup.find_all("div", class_="product-card", limit=10)
    
    for product in products:
        title_elem = product.find("h3")
        link_elem = product.find("a")
        desc_elem = product.find("p", class_="tagline")
        
        if title_elem and link_elem:
            articles.append({
                "title": title_elem.get_text(strip=True),
                "link": "https://www.producthunt.com" + link_elem["href"],
                "desc": desc_elem.get_text(strip=True) if desc_elem else "",
            })
    
    return articles
```

---

## ğŸ”§ æ¥å£è§„èŒƒè¯¦è§£

### å¿…éœ€æ¥å£

```python
def fetch() -> List[Dict[str, str]]:
    """
    ä»æ–°é—»æºè·å–æ–‡ç« 
    
    Returns:
        æ–‡ç« åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ç« ä¸ºå­—å…¸ï¼ŒåŒ…å«:
        - title (str): æ–‡ç« æ ‡é¢˜ (å¿…éœ€)
        - link (str): å®Œæ•´ URL (å¿…éœ€)
        - desc (str): ç®€çŸ­æè¿° (å¿…éœ€ï¼Œå»ºè®® 50-200 å­—)
    
    Raises:
        requests.RequestException: ç½‘ç»œè¯·æ±‚å¤±è´¥
        ValueError: æ•°æ®æ ¼å¼é”™è¯¯
    """
```

### è¿”å›æ ¼å¼ç¤ºä¾‹

```python
[
    {
        "title": "OpenAI å‘å¸ƒ GPT-5",
        "link": "https://example.com/article-1",
        "desc": "OpenAI ä»Šæ—¥å®£å¸ƒæ¨å‡º GPT-5 é¢„è§ˆç‰ˆï¼Œæ€§èƒ½æå‡ 50%..."
    },
    {
        "title": "Startup X å®Œæˆ B è½®èèµ„",
        "link": "https://example.com/article-2",
        "desc": "AI åˆåˆ›å…¬å¸ Startup X å®£å¸ƒå®Œæˆ 5000 ä¸‡ç¾å…ƒ B è½®èèµ„..."
    }
]
```

### é”™è¯¯å¤„ç†

```python
def fetch() -> List[Dict[str, str]]:
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        # ...
    except requests.RequestException as e:
        print(f"   âŒ Failed to fetch from {source_name}: {e}")
        return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
    except Exception as e:
        print(f"   âš ï¸  Unexpected error: {e}")
        return []
```

---

## ğŸ§ª æµ‹è¯•å’Œè°ƒè¯•

### å•å…ƒæµ‹è¯•

åˆ›å»º `tests/test_sources.py`:

```python
import pytest
from sources.hackernews import fetch


def test_hackernews_fetch():
    """æµ‹è¯• Hacker News æŠ“å–"""
    articles = fetch()
    
    # åŸºæœ¬éªŒè¯
    assert isinstance(articles, list)
    assert len(articles) > 0
    
    # éªŒè¯æ•°æ®ç»“æ„
    for article in articles:
        assert "title" in article
        assert "link" in article
        assert "desc" in article
        assert article["link"].startswith("http")


def test_hackernews_fetch_with_mock(monkeypatch):
    """ä½¿ç”¨ Mock æ•°æ®æµ‹è¯•"""
    class MockResponse:
        @staticmethod
        def json():
            return [1, 2, 3]  # Mock story IDs
        
        @staticmethod
        def raise_for_status():
            pass
    
    def mock_get(*args, **kwargs):
        return MockResponse()
    
    monkeypatch.setattr("requests.get", mock_get)
    
    articles = fetch()
    assert len(articles) == 3
```

**è¿è¡Œæµ‹è¯•**:

```bash
pytest tests/test_sources.py -v
```

### æ‰‹åŠ¨æµ‹è¯•

åˆ›å»º `test_single_source.py`:

```python
#!/usr/bin/env python
"""å¿«é€Ÿæµ‹è¯•å•ä¸ªæ–°é—»æº"""
import sys
from sources.hackernews import fetch


def main():
    print("Testing Hacker News source...")
    articles = fetch()
    
    print(f"\nâœ… Fetched {len(articles)} articles\n")
    
    for i, article in enumerate(articles[:5], 1):
        print(f"{i}. {article['title']}")
        print(f"   Link: {article['link']}")
        print(f"   Desc: {article['desc'][:80]}...")
        print()


if __name__ == "__main__":
    main()
```

**è¿è¡Œ**:

```bash
python test_single_source.py
```

---

## ğŸ› ï¸ å¸¸è§é—®é¢˜

### Q1: æŠ“å–çš„æ•°æ®ä¸ºç©º

**å¯èƒ½åŸå› **:
1. ç½‘ç«™ç»“æ„å˜åŒ– (HTML è§£æå¤±æ•ˆ)
2. API è®¿é—®é™åˆ¶
3. ç½‘ç»œé—®é¢˜

**è°ƒè¯•æ–¹æ³•**:

```python
# æ·»åŠ è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

response = requests.get(url)
print("Status Code:", response.status_code)
print("Content:", response.text[:500])  # æ‰“å°å‰ 500 å­—ç¬¦
```

### Q2: æ ‡é¢˜æˆ–æè¿°è¿‡é•¿

**è§£å†³**: åœ¨è¿”å›å‰æˆªæ–­

```python
def fetch():
    # ...
    articles.append({
        "title": title[:150],  # é™åˆ¶ 150 å­—ç¬¦
        "desc": desc[:300],    # é™åˆ¶ 300 å­—ç¬¦
    })
```

### Q3: ç‰¹æ®Šå­—ç¬¦å¯¼è‡´ç¼–ç é”™è¯¯

**è§£å†³**: ç¡®ä¿æ­£ç¡®å¤„ç†ç¼–ç 

```python
response = requests.get(url)
response.encoding = "utf-8"  # å¼ºåˆ¶ UTF-8
text = response.text
```

### Q4: è¯·æ±‚è¢«å°ç¦ (403/429)

**è§£å†³**: æ·»åŠ  User-Agent å’Œå»¶è¿Ÿ

```python
headers = {
    "User-Agent": "Mozilla/5.0 (compatible; DailyReport/1.0)"
}
response = requests.get(url, headers=headers, timeout=10)

# æ·»åŠ å»¶è¿Ÿ (é¿å…é¢‘ç¹è¯·æ±‚)
import time
time.sleep(1)
```

---

## ğŸ¨ æœ€ä½³å®è·µ

### 1. ä½¿ç”¨ç¼“å­˜ (å¯é€‰)

```python
import functools
import time

@functools.lru_cache(maxsize=1)
def fetch_cached():
    """å¸¦ç¼“å­˜çš„æŠ“å– (5 åˆ†é’Ÿæœ‰æ•ˆ)"""
    # å®é™…æŠ“å–é€»è¾‘
    return fetch()

def fetch():
    # æ£€æŸ¥ç¼“å­˜æ—¶é—´
    current_time = time.time()
    # ... (ç¼“å­˜é€»è¾‘)
    return fetch_cached()
```

### 2. é™æµå™¨

```python
import time
from threading import Lock

class RateLimiter:
    def __init__(self, calls_per_second=1):
        self.calls_per_second = calls_per_second
        self.lock = Lock()
        self.last_call = 0
    
    def wait(self):
        with self.lock:
            elapsed = time.time() - self.last_call
            wait_time = (1.0 / self.calls_per_second) - elapsed
            if wait_time > 0:
                time.sleep(wait_time)
            self.last_call = time.time()

limiter = RateLimiter(calls_per_second=2)

def fetch():
    limiter.wait()
    # å®é™…è¯·æ±‚
```

### 3. ç»“æ„åŒ–æ—¥å¿—

```python
import logging

logger = logging.getLogger(__name__)

def fetch():
    logger.info("Starting fetch from Hacker News")
    try:
        # ...
        logger.info(f"Successfully fetched {len(articles)} articles")
        return articles
    except Exception as e:
        logger.error(f"Failed to fetch: {e}", exc_info=True)
        return []
```

---

## ğŸ“¦ å®Œæ•´ç¤ºä¾‹: Reddit Subreddit

```python
# sources/reddit.py
"""
Reddit Subreddit Scraper (using PRAW library)
"""
import os
import praw
from typing import List, Dict


def fetch() -> List[Dict[str, str]]:
    """ä» r/technology è·å–çƒ­é—¨å¸–å­"""
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    
    if not client_id or not client_secret:
        print("   âš ï¸  Reddit credentials not configured")
        return []
    
    reddit = praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent="DailyReport/1.0"
    )
    
    subreddit = reddit.subreddit("technology")
    articles = []
    
    for post in subreddit.hot(limit=15):
        if post.is_self:  # è·³è¿‡è‡ªåŠ©è´´
            continue
        
        articles.append({
            "title": post.title,
            "link": post.url,
            "desc": post.selftext[:200] if post.selftext else f"{post.score} upvotes",
        })
    
    return articles
```

**å®‰è£…ä¾èµ–**:

```bash
pip install praw
```

**é…ç½®** (`.env`):

```bash
REDDIT_CLIENT_ID=your-client-id
REDDIT_CLIENT_SECRET=your-client-secret
```

---

## ğŸ”— ç›¸å…³èµ„æº

- [requests æ–‡æ¡£](https://docs.python-requests.org/)
- [BeautifulSoup æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [feedparser æ–‡æ¡£](https://feedparser.readthedocs.io/)
- [API å‚è€ƒæ–‡æ¡£](../api/README.md)

---

## ğŸš€ ä¸‹ä¸€æ­¥

- [é…ç½®æ–‡ä»¶è¯¦è§£](configuration.md)
- [æ•…éšœæ’æŸ¥æ‰‹å†Œ](troubleshooting.md)
- [æŸ¥çœ‹ CONTRIBUTING è§„èŒƒ](../../CONTRIBUTING.md)

---

**Last Updated**: 2026-01-21
